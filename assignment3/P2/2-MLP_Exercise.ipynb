{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <b>Machine Learning - SBU FALL 2024</b></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "Student_Number = '400243059'\n",
    "Name = 'Mohammad Moein'\n",
    "Last_Name = 'Arabi'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXl9VKKjaTOr"
   },
   "source": [
    "In this notebook, you are expected to implement a fully functional MLP (Multi-Layer Perceptron) neural network from scratch. \n",
    "You are not allowed to use any libraries (including numpy). You will use the **Iris dataset** for training and testing your network, focusing on reducing the error on this dataset. \n",
    "\n",
    "\n",
    "**modify iris dataset to a version compatible with this task : binary classifiaction of if a flower is setosa (1) or not(-1)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cdv3E5_Fe_27"
   },
   "source": [
    "# Automatic Differentiation\n",
    "\n",
    "Automatic differentiation has two main methods: forward mode and reverse mode. \n",
    "PyTorch uses the reverse mode approach, and we will also use this method in this project.\n",
    "\n",
    "To learn this concept, simply click on this [link](https://auto-ed.readthedocs.io/en/latest/mod3.html#i-the-basics-of-reverse-mode) \n",
    "and read only the section \"Intuition for Example An IV\" up to the end of step six.\n",
    "\n",
    "Essentially, you need to consider a data structure to build a computational graph. \n",
    "By calling the `backward` function on the network's output, you can compute the derivative of the output \n",
    "with respect to all weights and biases of the network. (In this case, our network has only one output.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_o2mBT0MAc5T"
   },
   "source": [
    "## Visualization Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "bHSUQ91rpaBc"
   },
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "import random\n",
    "import math\n",
    "from math import exp\n",
    "\n",
    "def trace(root):\n",
    "    nodes, edges = set(), set()\n",
    "\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v.children:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "\n",
    "def draw_dot(root, format='svg', rankdir='LR'):\n",
    "    \"\"\"\n",
    "    format: png | svg | ...\n",
    "    rankdir: TB (top to bottom graph) | LR (left to right)\n",
    "    \"\"\"\n",
    "    assert rankdir in ['LR', 'TB']\n",
    "    nodes, edges = trace(root)\n",
    "    dot = Digraph(format=format, graph_attr={'rankdir': rankdir})  # , node_attr={'rankdir': 'TB'})\n",
    "\n",
    "    for n in nodes:\n",
    "        dot.node(name=str(id(n)), label=\"{ %s | data %.4f | grad %.4f }\" % (n.label, n.value, n.grad), shape='record')\n",
    "        if n.operator:\n",
    "            dot.node(name=str(id(n)) + n.operator, label=n.operator)\n",
    "            dot.edge(str(id(n)) + n.operator, str(id(n)))\n",
    "\n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2.operator)\n",
    "\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jis6y_bKE7OG"
   },
   "source": [
    "## Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "2ecztx1Jkili"
   },
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "  def __init__(self, value, label='', children=set(), operator=None):\n",
    "    self.value = value\n",
    "    self.children = set(children)\n",
    "    self.operator = operator\n",
    "    self.grad = 0\n",
    "    self._backward = lambda: None\n",
    "    self.label = label\n",
    "\n",
    "  def __repr__(self) -> str:\n",
    "    return f\"Tensor(label = {self.label}, value = {self.value}, grad = {self.grad}, operator = {self.operator})\"\n",
    "\n",
    "  def __add__(self, other):\n",
    "    other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "    out_value = self.value + other.value\n",
    "    out = Tensor(out_value,'Add' ,children=(self, other), operator='+')\n",
    "    def backward():\n",
    "        self.grad = 1 * out.grad\n",
    "        other.grad = 1 * out.grad\n",
    "    out._backward = backward\n",
    "    return out\n",
    "\n",
    "  def __radd__(self, other):\n",
    "    return self + other\n",
    "\n",
    "  def __sub__(self, other):\n",
    "    other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "    out_value = self.value - other.value\n",
    "    out = Tensor(out_value, 'Sub', children=(self, other), operator='-')\n",
    "    def backward():\n",
    "        self.grad = 1 * out.grad\n",
    "        other.grad = -1 * out.grad\n",
    "    out._backward = backward\n",
    "    return out\n",
    "\n",
    "  def __mul__(self, other):\n",
    "    other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "    out_value = self.value * other.value\n",
    "    out = Tensor(out_value, 'Mul', children=(self, other), operator='*')\n",
    "    def backward():\n",
    "        self.grad = other.value * out.grad\n",
    "        other.grad = self.value * out.grad\n",
    "    out._backward = backward\n",
    "    return out\n",
    "\n",
    "  def __rmul__(self, other):\n",
    "    return self * other\n",
    "\n",
    "  def __truediv__(self, other):\n",
    "    other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "    out_value = self.value / other.value\n",
    "    out = Tensor(out_value, 'div', children=(self, other), operator='/')\n",
    "    def backward():\n",
    "        self.grad = 1 / other.value * out.grad\n",
    "        other.grad = -self.value / (other.value ** 2) * out.grad\n",
    "    out._backward = backward\n",
    "    return out\n",
    "\n",
    "  def __pow__(self, other):\n",
    "    other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "    out_value = self.value ** other.value\n",
    "    out = Tensor(out_value, 'pow', children=(self, other), operator='**')\n",
    "    def backward():\n",
    "        self.grad = other.value * (self.value ** (other.value - 1)) * out.grad\n",
    "        other.grad = (self.value ** other.value) * math.log(self.value) * out.grad\n",
    "    out._backward = backward\n",
    "    return out\n",
    "\n",
    "  def backward(self):\n",
    "    self.grad = 1\n",
    "    self._backward()\n",
    "    return self.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGC0u8U-Nefy"
   },
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "_ecFayfBy_EW"
   },
   "outputs": [],
   "source": [
    "class F:\n",
    "    @staticmethod\n",
    "    def tanh(x: Tensor) -> Tensor:\n",
    "        # print(x)\n",
    "        out_value = math.tanh(x.value)\n",
    "        out = Tensor(out_value, 'tanh', children=(x,), operator='tanh')\n",
    "        def backward():\n",
    "            x.grad = (1 - out.value ** 2) * out.grad\n",
    "        out._backward = backward\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        self.grad = 1\n",
    "        self._backward()\n",
    "        return self.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWebVMc6JMbc"
   },
   "source": [
    "# Neuron, Layer & MLP (Forward Section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "3pbRTBAjmng1"
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, input_size):\n",
    "        self.weights = [Tensor(random.uniform(-1, 1), label='weight') for i in range(input_size)]\n",
    "        self.bias = Tensor(random.uniform(-1, 1), label='bias')\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.bias\n",
    "        for i in range(len(self.weights)):\n",
    "            out += x[i] * self.weights[i]\n",
    "        return F.tanh(out)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.weights\n",
    "\n",
    "    def __print__(self):\n",
    "        return f'Neuron(weights = {self.weights}, bias = {self.bias})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "TIEV-ViUDk7j"
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.neurons = [Neuron(input_size) for _ in range(output_size)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return [neuron(x) for neuron in self.neurons]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def parameters(self):\n",
    "        return [param for neuron in self.neurons for param in neuron.parameters()]\n",
    "\n",
    "    def __print__(self):\n",
    "        return f'Layer(neurons = {self.neurons})'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74V1J3PGeTlm"
   },
   "source": [
    "# The MLP Class Structure\n",
    "\n",
    "The `MLP` class is expected to have three main methods, which you should implement with the same structure:\n",
    "\n",
    "1. **`forward` Method:**  \n",
    "   This method performs calculations on the input and returns the output.\n",
    "\n",
    "2. **`__call__` Method:**  \n",
    "   This method simply calls the `forward` method. Essentially, we want to pass the input to the model in this way: `model(x)`.\n",
    "\n",
    "3. **`parameters` Method:**  \n",
    "   This method returns all the weights of the network in a list.\n",
    "\n",
    "---\n",
    "\n",
    "# Layers in the MLP\n",
    "\n",
    "The `MLP` class itself consists of several layers (referred to as `Layer`), which are the actual layers of the neural network. Each layer needs to know:\n",
    "- The dimensions of the input data.\n",
    "- The dimensions of the output data.\n",
    "\n",
    "In this project, all inputs are vectors. Each `Layer` consists of several neurons. For example:\n",
    "- If a layer receives a 7-dimensional vector as input (input size) and produces a 4-dimensional vector as output, then:\n",
    "  - The layer should have 4 neurons.\n",
    "  - Each neuron should have 7 weights and 1 bias.\n",
    "\n",
    "You should implement the details and structure of the layers and neurons according to the given explanation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "5kB1TmeoDsPv"
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, input_size, layer_sizes):\n",
    "        layers_total = [input_size] + layer_sizes\n",
    "        self.layers = [Layer(layers_total[i], layers_total[i+1]) for i in range(len(layer_sizes))]\n",
    "\n",
    "    def forward(self, input_vector):\n",
    "        for layer in self.layers:\n",
    "            input_vector = layer(input_vector)\n",
    "        return input_vector\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def parameters(self):\n",
    "        return [param for layer in self.layers for param in layer.parameters()]\n",
    "\n",
    "    def __print__(self):\n",
    "        return f'MLP(layers = {self.layers})'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wIeXxs1gw4q"
   },
   "source": [
    "# Optimizer\n",
    "\n",
    "Similar to the first project, the `optimizer` should have access to the network's weights. \n",
    "This time, it must update them based on their derivatives and the value of the `lr` parameter (learning rate).\n",
    "\n",
    "- **`step` Method:**  \n",
    "  This method functions similarly to the `update` method in the previous project. It updates the weights of the network.\n",
    "\n",
    "- **`grad_zero` Method:**  \n",
    "  An additional method called `grad_zero` is included, whose functionality has been explained. It is used to reset the gradients of the weights to zero after each update step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "Og_hjDcedPsm"
   },
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, parameters, lr):\n",
    "        self.parameters = parameters\n",
    "        self.lr = lr\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.parameters:\n",
    "            param.grad = 0\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.parameters:\n",
    "            param.value -= self.lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vy-yOcZ2TfqP"
   },
   "source": [
    "# Training Part\n",
    "\n",
    "Prepeare the dataset in this section in bellow code snippet we place a toy example dataset just for better clarification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "\n",
    "Y = [1.0, -1.0, -1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                  5.1               3.5                1.4               0.2   \n",
       "1                  4.9               3.0                1.4               0.2   \n",
       "2                  4.7               3.2                1.3               0.2   \n",
       "3                  4.6               3.1                1.5               0.2   \n",
       "4                  5.0               3.6                1.4               0.2   \n",
       "..                 ...               ...                ...               ...   \n",
       "145                6.7               3.0                5.2               2.3   \n",
       "146                6.3               2.5                5.0               1.9   \n",
       "147                6.5               3.0                5.2               2.0   \n",
       "148                6.2               3.4                5.4               2.3   \n",
       "149                5.9               3.0                5.1               1.8   \n",
       "\n",
       "     target  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "..      ...  \n",
       "145       2  \n",
       "146       2  \n",
       "147       2  \n",
       "148       2  \n",
       "149       2  \n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris_df['target'] = iris.target\n",
    "iris_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[-1, 1]\n"
     ]
    }
   ],
   "source": [
    "MAX_TRUE = 1\n",
    "MAX_FALSE = 1\n",
    "index = set()\n",
    "while len(index) < MAX_TRUE:\n",
    "    index.add(random.randrange(0, 49))\n",
    "while len(index) < MAX_TRUE + MAX_FALSE:\n",
    "    index.add(random.randrange(50, len(iris.data)))\n",
    "\n",
    "iris_target = [1 if x == 0 else -1 for x in iris.target]\n",
    "iris_target = [iris_target[i] for i in index]\n",
    "print(len(iris_target))\n",
    "print(iris_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([5.9, 3. , 5.1, 1.8]), array([5.4, 3.4, 1.5, 0.4])]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data = [iris.data[i] for i in index]\n",
    "iris_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Tensor(label = data, value = 5.9, grad = 0, operator = None), Tensor(label = data, value = 3.0, grad = 0, operator = None), Tensor(label = data, value = 5.1, grad = 0, operator = None), Tensor(label = data, value = 1.8, grad = 0, operator = None)], [Tensor(label = data, value = 5.4, grad = 0, operator = None), Tensor(label = data, value = 3.4, grad = 0, operator = None), Tensor(label = data, value = 1.5, grad = 0, operator = None), Tensor(label = data, value = 0.4, grad = 0, operator = None)]]\n",
      "[Tensor(label = target, value = -1, grad = 0, operator = None), Tensor(label = target, value = 1, grad = 0, operator = None)]\n"
     ]
    }
   ],
   "source": [
    "iris_data = list(map(lambda x: [Tensor(i, label='data') for i in x], iris_data))\n",
    "iris_target = list(map(lambda x: Tensor(x, label='target'), iris_target))\n",
    "print(iris_data)\n",
    "print(iris_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwMIvkuWOGwW"
   },
   "source": [
    "## Loss Function (SE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "ux28TnpO1-aX"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def criterion(y_hats: List[Tensor], Y) -> Tensor:\n",
    "    return sum([(y_hat[0] - y)**2 for y_hat, y in zip(y_hats, Y)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOgU8n9bhIL_"
   },
   "source": [
    "# Training Steps for the Model\n",
    "\n",
    "1. **Calculate Model Predictions:**  \n",
    "   For each input `x`, compute the output of the model, `y_hat`.\n",
    "\n",
    "2. **Compute Error:**  \n",
    "   Calculate the error of the predictions using the Mean Squared Error (MSE) loss function for simplicity.\n",
    "\n",
    "3. **Reset Gradients of Network Variables:**  \n",
    "   Set the gradients of all variables in the network to zero. Once you implement automatic differentiation, you will understand why this step is necessary.\n",
    "\n",
    "4. **Compute Derivatives:**  \n",
    "   This is the most challenging part of the project. When this function is called, you need to calculate the derivative of the `loss` with respect to all weights and biases of the network.  \n",
    "   To implement this, you will use **Automatic Differentiation (AutoDiff)**.\n",
    "\n",
    "5. **Update Network Weights:**  \n",
    "   The `optimizer` will use the derivatives of the `loss` with respect to all weights and biases to update them in a direction that reduces the error in the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "NCAwwaS9Zwys"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Tensor(label = tanh, value = -0.08682754642946831, grad = 0, operator = tanh)], [Tensor(label = tanh, value = -0.3050341284623759, grad = 0, operator = tanh)]]\n",
      "[[Tensor(label = tanh, value = -0.08682754642946831, grad = 0, operator = tanh)], [Tensor(label = tanh, value = -0.3050341284623759, grad = 0, operator = tanh)]]\n",
      "[[Tensor(label = tanh, value = -0.08682754642946831, grad = 0, operator = tanh)], [Tensor(label = tanh, value = -0.3050341284623759, grad = 0, operator = tanh)]]\n",
      "[[Tensor(label = tanh, value = -0.08682754642946831, grad = 0, operator = tanh)], [Tensor(label = tanh, value = -0.3050341284623759, grad = 0, operator = tanh)]]\n",
      "[[Tensor(label = tanh, value = -0.08682754642946831, grad = 0, operator = tanh)], [Tensor(label = tanh, value = -0.3050341284623759, grad = 0, operator = tanh)]]\n",
      "[[Tensor(label = tanh, value = -0.08682754642946831, grad = 0, operator = tanh)], [Tensor(label = tanh, value = -0.3050341284623759, grad = 0, operator = tanh)]]\n",
      "[[Tensor(label = tanh, value = -0.08682754642946831, grad = 0, operator = tanh)], [Tensor(label = tanh, value = -0.3050341284623759, grad = 0, operator = tanh)]]\n",
      "[[Tensor(label = tanh, value = -0.08682754642946831, grad = 0, operator = tanh)], [Tensor(label = tanh, value = -0.3050341284623759, grad = 0, operator = tanh)]]\n",
      "[[Tensor(label = tanh, value = -0.08682754642946831, grad = 0, operator = tanh)], [Tensor(label = tanh, value = -0.3050341284623759, grad = 0, operator = tanh)]]\n",
      "[[Tensor(label = tanh, value = -0.08682754642946831, grad = 0, operator = tanh)], [Tensor(label = tanh, value = -0.3050341284623759, grad = 0, operator = tanh)]]\n",
      "[[Tensor(label = tanh, value = -0.08682754642946831, grad = 0, operator = tanh)], [Tensor(label = tanh, value = -0.3050341284623759, grad = 0, operator = tanh)]]\n",
      "[[Tensor(label = tanh, value = -0.08682754642946831, grad = 0, operator = tanh)], [Tensor(label = tanh, value = -0.3050341284623759, grad = 0, operator = tanh)]]\n",
      "[[Tensor(label = tanh, value = -0.08682754642946831, grad = 0, operator = tanh)], [Tensor(label = tanh, value = -0.3050341284623759, grad = 0, operator = tanh)]]\n",
      "[[Tensor(label = tanh, value = -0.08682754642946831, grad = 0, operator = tanh)], [Tensor(label = tanh, value = -0.3050341284623759, grad = 0, operator = tanh)]]\n",
      "[[Tensor(label = tanh, value = -0.08682754642946831, grad = 0, operator = tanh)], [Tensor(label = tanh, value = -0.3050341284623759, grad = 0, operator = tanh)]]\n",
      "[[Tensor(label = tanh, value = -0.08682754642946831, grad = 0, operator = tanh)], [Tensor(label = tanh, value = -0.3050341284623759, grad = 0, operator = tanh)]]\n",
      "[[Tensor(label = tanh, value = -0.08682754642946831, grad = 0, operator = tanh)], [Tensor(label = tanh, value = -0.3050341284623759, grad = 0, operator = tanh)]]\n",
      "[[Tensor(label = tanh, value = -0.08682754642946831, grad = 0, operator = tanh)], [Tensor(label = tanh, value = -0.3050341284623759, grad = 0, operator = tanh)]]\n",
      "[[Tensor(label = tanh, value = -0.08682754642946831, grad = 0, operator = tanh)], [Tensor(label = tanh, value = -0.3050341284623759, grad = 0, operator = tanh)]]\n",
      "[[Tensor(label = tanh, value = -0.08682754642946831, grad = 0, operator = tanh)], [Tensor(label = tanh, value = -0.3050341284623759, grad = 0, operator = tanh)]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "\n",
    "input_size = 4 # Number of features in the input layer\n",
    "layer_sizes = [2, 3, 1] # Number of neurons in each hidden and output layer\n",
    "model = MLP(input_size, layer_sizes)\n",
    "optim = Optimizer(model.parameters(), lr=0.01)\n",
    "\n",
    "for _ in range(n_epochs):\n",
    "    # Forward pass: Compute predictions for the entire dataset\n",
    "    y_hats = [model(x) for x in iris_data]\n",
    "    print(y_hats)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = criterion(y_hats, iris_target)\n",
    "\n",
    "    # Zero the gradients to prevent accumulation from previous iterations\n",
    "    optim.zero_grad()\n",
    "\n",
    "    # Backward pass: Compute the gradient of the loss function with respect to model parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the model parameters using the optimizer\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graph.png'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot = draw_dot(loss)\n",
    "# save to file\n",
    "dot.render('graph', format='png', cleanup=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
